研究内容
研究内容1：分层引导式帕累托搜索框架的构建与实现
构建一个基于决策逻辑的分层框架，并通过一种基于偏好向量的引导机制，将复杂的全局多目标问题转化为一系列有明确引导的局部单目标优化问题，从而实现高效的帕累托前沿搜索。

拟采取的方法、技术路线及可行性分析
1.分层引导式帕累托搜索框架的构建与实现
研究方法
采用分层多智能体强化学习（HMARL）方法，并内嵌基于动态权重标量化的引导机制。首先，将决策体系分解为观察全局、制定战略的战略层智能体和执行具体任务的战术层智能体。其次，通过设计一种基于偏好向量的引导机制来连接这两个层次。战术层智能体将利用战略层下发的偏好向量p，对其自身的多维度奖励r（包含效率、成本、交付期等）进行实时加权求和，从而将一个复杂的多目标问题转化为一个有明确引导的单目标问题进行求解，即R_tactical=p*r。
技术路线
第一步：将决策体系分解为“1（战略）+2（战术）”的三智能体分层架构，高层战略智能体负责制定偏好，底层生产智能体决定工件-机器匹配，底层物流智能体指挥AGV执行运输。底层智能体的动作空间被定义为离散的、细粒度的基础操作，并采用动作掩码技术来保证决策的有效性。构建一个统一的GNN将动态的车间状态图编码为包含丰富结构信息的嵌入向量，为所有三个智能体提供决策依据。
第二步：在仿真环境中，精确定义与效率、成本、交付期相关的奖励子函数。由于这三个奖励子函数的量纲和疏密程度不同，将对它们进行动态归一化处理，确保没有任何一个子目标在训练初期主导学习过程。
第三步：为战略、生产、物流三个智能体分别构建其独立的策略网络（Actor），每个Actor根据自身的观察和目标输出各自的动作。在训练时，构建一个单一的中心化Critic。该Critic能够访问全局状态信息以及全部三个智能体的动作，同时指导三个独立的Actor网络进行更新。每个Actor仅接一个小MLP头（如两层、隐藏128）或注意力池化后的小头，Critic用同一编码加全局池化进行状态拼接。
第四步：在战术层智能体的训练算法中，嵌入奖励标量化模块，使其能够根据接收到的偏好向量动态调整学习目标。同时，探索奖励塑形技术，通过引入潜能函数Φ(s)，为智能体提供更密集的中间信号，以缓解稀疏奖励问题，加速收敛。